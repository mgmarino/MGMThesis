\chapter{Analysis of a low-background BEGe detector}
\label{chap:AnalysisBeGe}
	\section{Experimental setup}
	\label{sec:BeGeExperimentalSetup}

A custom 440~g PPC \textbf{B}road-\textbf{E}nergy \textbf{G}ermanium (BEGe) detector manufactured by Canberra Industries was deployed underground on 24~August~2009 in the Soudan Underground Laboratory in Soudan, MN.  The purpose of the deployment of this detector was to obtain low-threshold, low-background counting data to seek improved limits on WIMPs in the low-mass range ($\lesssim10$~GeV).  The design of this detector included several modifications to address backgrounds seen with a previous PPC used to obtain dark matter limits~\cite{Aalseth:2008aa}, including several internal mounting components replaced by ones constructed with cleaner materials by J.~Collar at the University of Chicago.  
% Additionally, the outer cryostat was not made of typical aluminum but rather of copper.  True?
The detector was placed in the same shield as described in Section~\ref{sec:DeploymentPPC2SoudanIntro} with a few small changes.  The inner OFHC copper shield was replaced by old Pb bricks from the University of Chicago and a different PMT-based muon veto was placed around the external borated polyethylene shield.   %FixME contamination of bricks?

This chapter presents results from data taken with the BeGE detector underground from the time period beginning 4 December 2009 until 15 May 2010 for a total live-time of 150.6~days.  During this time, there were two main gaps in run time, including a roughly 5-day shutdown period during a planned power outage at the lab and another 6-day period during which parameter settings for the DAQ were modified.  Results from a analysis of a previous subset of the data occupying about 2 months of live-time have been presented in~\cite{Aalseth:2010aa}.

	\section{Data acquisition and processing}
	\label{sec:BeGeDAQProcessing}
	
The DAQ system used was similar to the one used in~\cite{Aalseth:2008aa}.  The preamp of the BEGe included 2~signal outputs, an inhibit output generating a logic signal when the reset circuitry of the preamp was active, and a test input for waveform generator pulses.  The initial system was setup without the ability to digitize raw preamp traces, but a later upgrade on 1~Dec~2009 introduced additional hardware to take this data.  The readout system was a PCI-based National Instruments digitizer with 6~channels, sampling at 20~MS/s with a resolution of 8~bits.  This digitizer allowed selectable voltage ranges, enabling higher resolution at lower energies.  The acquisition software used was a Windows- and Labview-based program designed by J.~Collar.  

One of the preamp signal outputs was run into an analog spectroscopy amplifier with a 10$\mu$s shaping time.  Two outputs from this amplifier were input into the digitizer with different voltage ranges: -0.05$\to$0.05~V (high gain) and -0.25$\to$0.25~V (low gain).  The second preamp output was AC-coupled to a Phillips Scientific 777 fast (DC$\to$200~MHz) amplifier using a capacitor to yield a $\sim$50~$\mu$s decay time.  This stage provided a $\sim1\times$ gain and was used as a fan-out for two outputs to the next stage: (1) one output was input into a spectroscopy amplifier with 6~$\mu$s shaping time and then into the digitizer; (2) the other output was sent to another gain stage of the 777.  The two outputs from the fan-out of this stage were then input into two channels of the digitizer with different voltage ranges: -0.05$\to$0.05~V (high gain) and -0.25$\to$0.25~V (low gain).  The 777 allowed adjustment of the output offset, and so the baseline was set to be close to the upper maximum of the high-gain channel.  This was to maximize the dynamic range for digitizing the negative-going preamplifier pulses.  

The muon veto was composed of 10~flat panels situated around the outside of the polyethylene shield, with 6 on the sides and 4 covering the top.  The outputs of all the panels were coupled together and reduced to one channel, effectively OR-ing all the PMTs.  This single channel was then input into a discriminator with a threshold set to output logic pulses when any of the PMTs registered single photon.  This logic output was input into the last channel of the digitizer.  The outputs read into the DAQ system are detailed in Table~\ref{tab:SoudanDAQTable}.  All channels were digitized at 20~MHz with 8 bits, and each trace length was 8000 samples (400~$\mu$s) long.  The level-sensitive trigger was generated by the high-gain, 6~$\mu$s-shaped, channel~0.  When a pulse exceeded the trigger level in channel~0, all 6 channels were read out.  

	\begin{table}
		\centering
		\begin{tabular}{|l|c|}
			\hline
			Channel & Characteristics \\
			\hline
			\hline
			0 & High-gain (0-3.5~keV), 6$\mu$s shaping \\
			\hline
			1 & High-gain (0-3.5~keV), 10$\mu$s shaping, triggering \\
			\hline
			2 & Low-gain (0-14~keV), 10$\mu$s shaping \\
			\hline
			3 & Muon-veto \\
			\hline
			4 & High-gain, AC-coupled preamp trace (unshaped) \\
			\hline
			5 & Low-gain, AC-coupled preamp trace (unshaped) \\
			\hline
		\end{tabular}
		\caption[DAQ Channel readout characteristics]
		{DAQ Channel readout characteristics. }
		\label{tab:SoudanDAQTable}
	\end{table}

The inhibit output from the preamp was used as online veto by splitting the inhibit signal and using the `INHIBIT' inputs on the spectroscopy amplifiers.  When the logic is active on this INHIBIT input, the spectroscopy amplifier maintains the baseline of its output, effectively removing any trigger generated by a reset of the preamp.  It should be noted that, in contrast to the DAQ system described in Section~\ref{sec:DeploymentPPC2SoudanDAQSystem}, no timing information of the inhibit pulse was retained to perform a later estimate of deadtime.  Instead, during detector deployment it was found that the reset pulse rate was stable at $\lesssim$3~Hz, suggesting that differences in temperature did not affect this setup as profoundly as PPC2 (see Section~\ref{sec:DeploymentPPC2SoudanAnalysisCuts} or that the detector cold finger was more properly coupled to the liquid nitrogen.  %FixME Found out if JO still sees this at PNNL.

	The digitizer card maintained an internal buffer to store a set of events.  After 20~events were stored, data from the digitizer buffer were written to disk and files were cycled with name changes every 3 hours.  An automatic data management chain was setup similar to that described in Section~\ref{sec:DeploymentPPC2DataProcessing} with a run database used to facilitate the data processing.  Files were synchronized back to a server at the University of Washington where they were persisted on RAID-ed disks.  Once a file appeared on the UW server, a corresponding record was introduced into the database and this record was used to control and track further processing.  The processing progressed in a tiered fashion as follows:
		\begin{enumerate}
			\item[Tier 0:]  Raw data from the Labview DAQ system
			\item[Tier 1:]  ROOTified data - raw data converted to MGDO objects and stored in ROOT TFiles
			\item[Tier 2:]  Waveform processed data - extraction of waveform characteristics using MGDO Transforms
		\end{enumerate}
Details on the waveform processing are given in the following Sections: \ref{sec:BegeShapedProc}, \ref{sec:UnshapedWFProc}, and~\ref{sec:MuonProc}.  For more details regarding the MGDO objects and the framework of the analysis chain, see Appendix~\ref{app:MGDO}.
	
		\subsection{Shaped channel processing}
		\label{sec:BegeShapedProc}

The shaped channels (0, 1, and 2) were processed to extract amplitude information of the pulses.  These traces were first run through a 100~kHz low-bandpass filter to remove high-frequency noise and artifacts from the limited bit-depth of the digitizer.  Both extrema values, maximum and minimum, of each waveform were recorded and the baseline of each waveform was calculated using the first 280 $\mu$s (5600 samples).  The maximum was calculated both before and after the bandpass filter and both values saved; the minimum value was found on the unfiltered waveform.  The amplitude could then be calculated as the difference between the baseline and the maximum measured on the filtered waveform.  The minimum was saved to later analyze pulse health.

		\subsection{Unshaped channel processing}
		\label{sec:UnshapedWFProc}

The unshaped channels (4 and 5) were processed to extract information on the characteristics of the waveform, including baseline, extrema values, and rise-time information.    The baseline was calculated using the first 5600 samples and the maximum and minimum were found for each pulse.  Calculating the rise-time of each pulse required de-noising using a time-invariant stationary wavelet transformation (SWT).  This process is described later in Section~\ref{sec:RisetimeCuts}.  Extracting the extrema values -- the maximum and minimum of the waveform -- required running the waveform through a 100~kHz low-bandpass filter.  The filter was run on an unmodified waveform and \emph{not} after any other de-noising was done.  After the application of this filter, the maximum and minimum values and positions were recorded.  

		\subsection{Muon-veto channel}
		\label{sec:MuonProc}

The muon-veto channel digitized the logic output from a muon veto.  To process these waveforms, all the regions of each trace were found when the veto was logic positive.  Saving this information enabled any cut based upon the muon veto to be performed further down the analysis chain, retaining flexibility in determining parameters for such a cut.  However, due to the high count rate of the muon veto and the reduction in live-time any cut from this veto would create, it was decided to not use any cuts based upon results from this channel in the analysis.  


	\section{Data analysis}
	\label{sec:BeGeDataAnalysis}
	
		\subsection{Triggering efficiency}
		\label{sec:BeGeTrigEff}

The trigger efficiency of the DAQ system was measured by scanning a pulser of known amplitude through the threshold.  This measurement determined the ability of the DAQ electronics to trigger at certain signal amplitude given the noise characteristics of the preamplifier and the readout electronics.  The data was then fit to the function:
			\[
			1 - \frac{\operatorname{erf} \left(s(V-\sigma)\right)}{2}
			\]
where $V$ is in volts, $s$ is a scaling parameter and $\sigma$ is a position parameter.  This yielded the results: $\sigma = 7.137\times10^{-3}\pm8\times10^{-6}$ and $s = -1.772\times10^{3}\pm28$.  Results are shown in Figure~\ref{fig:BeGeTriggeringEfficiency}.  This study was done during initial deployment and not throughout the experiment due to concerns that noise or stray signals from the pulser could negatively affect the performance of the detector.  However, results from the previous deployment of a PPC detector and the monitoring of detector parameters versus time (Section~\ref{sec:PPC2DetParsVsTime}) suggested that other parameters could provide information to probe whether or not the trigger efficiency changed over time.  Such parameters include waveform characteristics (baseline, extrema) and triggering rates and these are discussed in Section~\ref{sec:BeGeTimeCorrelations}.

			\begin{figure}
				\centering
				\includegraphics[width=0.9\textwidth]{energy_efficiency_bege}
				\caption[BeGe triggering efficiency measured with a pulser]
				{BeGe triggering efficiency measured with a pulser.  Error bars are binomial.}
				\label{fig:BeGeTriggeringEfficiency}
			\end{figure}

		\subsection{Resolution of results}

FixME - Flesh out measurement of widths of gaussians

The inherent noise was measured using a pulser as well as x-ray lines.  The results of these measurements were folded into the following equation:

			\begin{equation}
				\sigma = \sqrt{\sigma_{elec}^{2} + E \eta F}
				\label{eqn:SigmaEqn}
			\end{equation}

to measure the intrinsic electronic noise $\sigma_{elec}$ and estimate the Fano factor $F$.  $E$ is the energy in keV, $\eta$ the amount of energy required to generate an electron-hole pair (2.96 eV).  $\sigma_{elec}$ was found to be $70.48\pm0.54$~eV and $F$ was estimated as $0.166\pm0.014$.  Results are shown in Figure~\ref{fig:BeGeResPlot}.

			\begin{figure}
				\centering
				%\includegraphics[width=0.8\textwidth]{Figures/resolution_plot1}
				\caption[BeGe resolution versus energy]
				{BeGe resolution versus energy.  Red is measured and fit intrinsic electronic noise, 
				black points are measured resolution from x-ray lines, black line is a fit to 
				Eqn.~\ref{eqn:SigmaEqn}.}
				\label{fig:BeGeResPlot}
			\end{figure}

		\subsection{Energy calibration}

Energy calibration at low energies is complicated due to several factors.  In particular, low-energy x-ray peaks (<10~keV) from any source will be heavily attenuated by the source itself as well as materials between the source and the crystal, including the outer cryostat, mounting components, and the crystal dead layer.  Higher energy x-rays have a larger probability to interact in the crystal, but these are unsuitable for calibration because of their distance from the signal region which, in the case of dark matter, is close to threshold.  However, internal cosmogenic isotopes provide excellent candidates for calibration since there are several with characteristic lines near or below 10~keV.  Therefore, the energy calibration was determined by fitting the high-energy channel simultaneously to the peaks listed in Table~\ref{tab:XRayLines} yielding a linear equation:

			\[
			E_{ion} (keV) = a V + b
			\]  

with $a = 63.81\pm0.25$~(keV/V) and $b = -0.014551\pm0.016$~keV.  These results were applied to both the low- and high-energy channels.  

		\subsection{Baseline versus energy}
		\label{sec:BeGeBaseline}

Describe the baseline versus energy and the features we see in the data set.  Maybe not?


	\section{Data cleaning and cuts}
	\label{sec:BeGeCuts}	
	
	Several cuts were employed to clean the data, removing spurious events from electronics noise or microphonics.  Additionally, other cuts were employed to remove events with slow rise-time.  This section discusses the implementation and results of \emph{all} cuts performed on the data set.  
	
		\subsection{Microphonics and Noise Cuts}
	     	\label{sec:MicroCuts}	
	
	Vibrations in detector components, such as the cryostat, cold-finger, or crystal mount, can induce electronic signals due to the changing capacitance created by the movement of components.  These electronic deviations can generate extra noise at low energies (threshold to a few keV) possibly obscuring signal in that region.  During LN filling of the dewar, it is possible to generate a significant amount of microphonics noise from vibration caused by the influx of new liquid nitrogen.  Therefore, during these filling procedures, a flag was set in the data to enable later removal of events occurring during a fill.  This flag was then lowered 5~minutes after the fill completed, resulting in a veto time of $\sim$15~minutes.  
		
	For microphonics induced from sources other than LN fills, another cut was made.  Morales et al.~developed a technique to mitigate this class of events, taking advantage of the fact that microphonics tend to have characteristics (e.g.~rise-time, fall-time, baseline shift) significantly different from events arising from charge collection in the crystal~\cite{Morales1992410}.  This procedure analyzes the ratio of amplitudes from two signal channels with different shaping times  and accepts or rejects events based upon their deviation from the expected ratio.  This expectation can be determined by using a source or a pulser at low amplitudes and in a setup where the amplification of the two shaped channels is nearly equivalent the expectation value of the ratio is close to 1.  
	
	In this application, a pulser was used to train the microphonics cut by taking high-statistics pulser runs at several discrete amplitudes near the threshold.  At these discrete amplitudes, the ratios of the two channels were histogrammed and fit to a gaussian.  The cut points were then determined by taking the values $\mu \pm 3 \sigma$ to accept 99.7\% of the events.  Since these cut points were estimated at discrete amplitudes, a 4th-order polynomial was fit to them to interpolate between the points.  The results of this are shown in Figure~\ref{fig:RatioOfShapedChannels}.  To avoid errors from the interpolation, the cut was softened forcing the upper limit to be no less than 1, and the lower limit to be no less than 0.8.  The final cut is then shown in Figure~\ref{fig:RatioOfShapedChannelsFinal} which shows an overlay of the calculated cut on data from a scanned pulser run (Figure~\ref{fig:RatioScannedPulser} and from the run data (Figure~\ref{fig:RatioData}).  Interesting to note is how the run data and the scanned pulser data follow a different distribution at energies above 1~keV, in particular that the scanned pulser data is centered around a ratio of 0.9 and the run data is centered at a lower value.  This suggests that the difference between a pulser and an actual crystal event become more significant at these energies.

			\begin{figure}
				\centering
				\includegraphics[width=0.8\textwidth]{Pulser_cut6}
				\caption[Calculation of microphonics cuts]
				{Calculation of microphonics cuts using the ratio of outputs from two different spectroscopy amplifiers with an input pulser at discrete amplitudes.  Line is 
				an estimate of the cut, see text for details.}
				\label{fig:RatioOfShapedChannels}
			\end{figure}

			\begin{figure}
				\centering
				\subfigure[Scanned pulser run]{
					\includegraphics[width=0.46\textheight]{pulser_scan_with_cuts1}
					\label{fig:RatioScannedPulser}						
				}												
				\subfigure[Data]{
					\includegraphics[width=0.46\textheight]{Ratio_of_amplitudes__Chan_0_Chan_1__vs_Energy__keV_}
					\label{fig:RatioData}						
				}				
				\caption[Microphonics cuts on data and scanned pulser runs]
				{Ratio of two shaped channels versus energy for a scanned pulser run and for run data.  The drawn line is the cut at 99.7\% efficiency, calculated using
				high-statistics pulser runs.}
				\label{fig:RatioOfShapedChannelsFinal}
			\end{figure}
	
	
		\subsection{Electronics cuts}
		\label{sec:BeGeElecCuts}
	
	The class of cuts intended to remove events coming from spurious electronic events which were distinctly not signal events.  Only two electronics cuts were applied: the first to remove events arising from uncaught reset events in the preamp and negative-going pulses, the second to remove noise pulses that arrived during a particular interval of the run time between 15 February 2010 and 15 March 2010.  
	
	The first cut removed events with a minimum in the shape channel below a certain voltage value chosen to be -0.02~V.  The population of events excluded by this cut included uncaught reset pulse events, negative-going events, and large energy depositions resulting in significant undershoot on the baseline.  Most of these events also saturated the digitizer positively, but many of the negative-going events did not.  A full explanation for these events was not determined though it is possible that they arise from breakdowns 
	
	While running tests analyzing the rates of events in different energy regions (see Section~\ref{sec:BeGeRate}), a class of events of unknown origin was found that occurred only during a sub-interval of the run time from 15 February 2010 until 15 March 2010.  These events were distinctive from true events, but were problematic because they populated the spectrum near threshold around 0.5~keV.   An example of such a pulse is given in Figure~\ref{fig:OddPulseExample}.  There were distinguishable by comparing the difference between the extrema (maximum - minimum) of the unshaped waveforms.  A plot of this parameter versus energy is given in Figure~\ref{fig:OddPulseCut} where the dashed line denotes the region of parameter space populated by these pulses.  
	
	To study the origin of the pulses, a rate analysis similar to that performed in Section~\ref{sec:BeGeRate} was applied to the subset of events.  The time between these events was histogrammed and fit to an exponential; the results are included in Figure~\ref{fig:OddPulseRate}.  This revealed that the events arrived not with a defined frequency, but rather in a Poisson fashion with a rate estimated from the fit as $2.97 \pm 0.1$ counts/hour.  This can be compared to the normal rate for this energy region $0.5\to1$~keV of $\sim0.124$ counts/hour (see Section~\ref{sec:BeGeRate}).  The detector was removed from high voltage during a power-down period of Soudan Lab (15 March -- 19 March 2010) and when bias was reapplied the noise pulses were no longer present.  It is not known what specifically changed during this period of time, though it is not expected that these pulses arose from any breakdown of the crystal.  Instead, the structure of the pulse suggests an induced signal possibly coming from a noise source capacitively-coupled to the signal lines.  This class of events was entirely removed using the discriminating line in Figure~\ref{fig:OddPulseCut}.
	
			\begin{figure}
				\centering
				\includegraphics[width=0.8\textwidth]{ExampleOddPulse}
				\caption[Example of noise pulse of unknown origin]
				{Example of noise pulse of unknown origin.  Energy specified is the energy this event is decoded as, indicating that these events can populate
				 near threshold.}
				\label{fig:OddPulseExample}
			\end{figure}	

			\begin{figure}
				\centering
				\includegraphics[width=0.8\textwidth]{Channel_5_Max_-_Min__V__vs_}
				\caption[Difference in unshaped waveform extrema versus energy]
				{Difference in unshaped waveform extrema (maximum - minimum) versus energy.  The dashed line is an indication of the cut used to exclude the 
				noise events removing all events within the selected region. }
				\label{fig:OddPulseCut}
			\end{figure}

			\begin{figure}
				\centering
				\includegraphics[width=0.8\textwidth]{odd_pulsesExponential_0.4_1}
				\caption[Time between noises pulses]
				{Time between noises pulses, the line is a fit to an exponential.  The events at longer time intervals are `contaminations' from true events arriving at a
				much slower rate.}
				\label{fig:OddPulseRate}
			\end{figure}
				
		\subsection{Risetime cuts}
	     	\label{sec:RisetimeCuts}	

	Pulses of slow rise-time were found during the deployment of PPC2 (see Section~\ref{sec:DeploymentPPC2SoudanAnalysisRisetime}).  These pulses were predominately at low energy near threshold which meant that they could compose a possible background to any signal in this region.  This section studies the methods developed to measure the rise-times at low signal-to-noise ratios against threshold and explores systematics related to a cut on this quantity.  Additionally, the likely origin of these pulses and future tests for for further exploration are discussed. 
	
			\subsubsection{Wavelet denoising}
		     	\label{sec:RisetimeCutsWaveletDenoise}
					
	As the amplitude-to-noise ratio of waveforms shrinks at low energies, calculating the rise-time becomes more sensitive to the magnitude of the noise.   Denoising via a simple bandpass filter is undesirable when both signal and noise are distributed across similar frequency bands as the signal-to-noise ratio will not be enhanced by removing particular frequencies.  When calculating the rise-time of a pulse, a bandpass filter can greatly attenuate the high frequencies present in the rising edge of the pulse.  Wavelet shrinkage provides methodology to reduce noise on a generic function (see e.g.~\cite{Don95aa,Don95bb}) when the function and and noise occupy the same frequency space.  The algorithm follows:
				\begin{enumerate}
					\item Choose a wavelet basis.
					\item Perform a wavelet transformation using the chosen basis to a level $n$, 
					obtaining $n$ sets of detail and approximation coefficients.
					\item Apply thresholding to the detail coefficients.
					\item Perform an inverse transformation.
				\end{enumerate}
	In this particular application it is necessary to use a translation-invariant version of the wavelet transformation called a Stationary Wavelet Transformation (SWT) (see~\cite{Coif95aa,Naso95aa}).  The SWT performs transformations at all possible translations for a given data set and basis wavelet.  A subsequent inverse SWT effectively averages these together, avoiding artifacts induced by any chosen origin.  
	
	For this wavelet analysis, the python package PyWavelets~\cite{PyWave} was used.  Since an implementation of the inverse SWT was missing from this distribution, the necessary extension to the package was written (see Section~\ref{sec:WaveformProcMGDO}.  A Haar wavelet was chosen as a basis wavelet due to its simplicity and asymmetry.  Thresholds for each set of detail coefficients were calculated using a pure-noise waveform training set of length $j$.  For each noise waveform $x^{(i)}$, a 6-level SWT was used to generate $D_{n}^{(i)}$ and the thresholds were calculated for each level $n$ according to the equation proposed by Donoho and Johnston~\cite{Don95ad}:
	
				\begin{equation}			
					\tau_{n}^{(i)} = \sigma_{n}^{(i)} \sqrt{2 \log N^{(i)}}
				\end{equation}			
				\[
					\sigma_{n}^{(i)} = \frac{\operatorname{MAD}\left(D_{n}^{(i)}\right)}{0.6745}
				\]
with $N^{(i)}$ the length of waveform, $x^{(i)}$, and MAD is the median average deviation.  The threshold at a level $n$ was then defined as $\tau_{n} = 0.8 \max(\tau_{n}^{(0)},...,\tau_{n}^{(j)})$.  An example of the coefficients calculated using a 6-level SWT is shown in Figure~\ref{fig:RisetimeCutsWaveletDecompositionOfPulse}.  This figure also includes the thresholds calculated at each level, denoted by dashed lines.  

Noise reduction was implemented by applying hard thresholding to each set of detail coefficients.  In this technique, all coefficients $D_{n}$ with an absolute value less than the threshold $\tau_{n}$ were set to zero.  Coefficients above this threshold value were unchanged.  The resultant coefficients were then used in an inverse SWT to produce a de-noised waveform.  An example of the wavelet de-noising is shown in the top figure of Figure~\ref{fig:RisetimeCutsExampleOfPulse}.
	
			
				\begin{figure}
					\centering
					\includegraphics[width=0.95\textwidth]{ExampleWavelet}
					\caption[Example wavelet decomposition of pulse]
					{Example wavelet decomposition of pulse in Figure~\ref{fig:RisetimeCutsExampleOfPulse}.  
					A$_{1}$ denotes the first-level
					 approximation coefficients, the D$_{n}$ denote the detail coefficients at the $n$th level of the 6-level stationary wavelet 
					 transformation.  Dashed lines indicate the thresholding used for each set of detail coefficients.}
					\label{fig:RisetimeCutsWaveletDecompositionOfPulse}
				\end{figure}					

			\subsubsection{Risetime calculation}
			\label{sec:RisetimeCalculation}
	The denoising process ensured that the waveforms be ready for rise-time calculations.  After de-noising, the smoothed derivative of the waveform was generated using a Savitzky-Golay derivative filter~\cite{Sav64aa}.  The extremum of the derivative - in this case the minimum since the pulse was negative-going - was then found and used to determine the middle of the rising edge, $p_{m}$.  The full-width at half maximum (FWHM) was calculated and used to estimate the beginning and end of the rise of the waveform: the beginning, $p_{b} = p_{m} - 1.5\times$FWHM and the end, $p_{e} = p_{m} + 1.5\times$FWHM.  The baseline and amplitude of the pulse were each found by averaging over 1~$\mu$s (20~samples) beginning at $p_{b} - (1~\mu$s) and $p_{e}$, respectively.  These values were used to estimate the amount of time it took the pulse to rise 10\%$\to$90\% in amplitude.  Linear interpolation was used to refine the time values which came between digitization points.  An example of this calculation, using the same pulse as in Figure~\ref{fig:RisetimeCutsWaveletDecompositionOfPulse}, is shown in Figure~\ref{fig:RisetimeCutsExampleOfPulse}.  
		
				\begin{figure}
					\centering
					\includegraphics[width=0.9\textwidth]{ExampleWaveform0}
					\caption[Example of rise-time calculation technique applied to a preamp trace]
					{Example of rise-time calculation technique applied to a preamp trace.  
					The top shows the raw and denoised waveforms, and the vertical dashed lines represent the result 
					of the rise-time calculation.  The bottom plot shows the smoothed derivative of the trace calculated 
					using a Savitzky-Golay filter~\cite{Sav64aa} of degree~2, width~6.}
					\label{fig:RisetimeCutsExampleOfPulse}
				\end{figure}					

			\subsubsection{Risetime simulation}
			\label{sec:RisetimeSimulation}
	
	To investigate how a cut based upon rise-time affected the spectrum, it was necessary to perform a simulation of the rise-time calculation on waveforms similar to the data.  The idea was to produce waveforms with similar characteristics (i.e. rise-time, noise) as seen in the detector and run them through the same algorithm used to analyze the detector data.  The waveform was generated by taking a tail pulse of 0~rise-time and running it through a digital low-pass RC filter.  The RC constant in the filter was tuned to reproduce the rise-time of ``fast'' pulses seen in the data.  In general this would not precisely reproduce all the characteristics of the detected pulses, but since the only parameter of interest was the rise-time it was reasonable to choose such a simple pulse construction.  Later systematic tests (Section~\ref{sec:RisetimeSystematicTests}) verified that it was sufficient.  
	
	The electronic noise of the detector was measured by looking at the baseline of all pulses and taking the average of power spectra for each preamp trace channel.  Once the average power spectrum was determined, it was possible to use this to add noise to the simulated pulse through the techniques outlined in~\cite{WanThesis08} by Wan Tseung.  Essentially, a measurement of an average power spectrum, $\Omega = X^{2} + Y^{2}$ where $X$ and $Y$ are the real and imaginary components of the Fourier Transform, gives you an average value $\mu_{i}$ at a frequency bin $i$.  If there is no phase information in the noise (i.e.~$\tan^{-1} (X_{i}/Y_{i})$ is flatly distributed), $X_{i}$ and $Y_{i}$ are gaussian-distributed variables around 0 with the same standard deviation, $\sigma_{i}$, which is related to the average value of bin $i$ via  $\mu_{i} = 2 \sigma_{i}^{2}$.  Therefore, for each simulated pulse, a noise waveform was generated in frequency space, transformed to the time domain using a discrete inverse Fourier Transform, and added to the original simulated pulse.  
	
	Since the energy of each event was determined using the amplitude of shaped pulses, it was necessary to determine a relationship between amplitudes of the shaped and unshaped low-gain channels (channel 1 and channel 4) and the amplitudes of the shaped and unshaped high-gain channels (channel 2 and channel 5).  This was done by fitting the relationship from data, an example of which is shown in Figure~\ref{fig:Risetimechan2vschan4}.  Additionally, the waveforms' position in the trace window exhibited a dependence on the energy of the event: for events of smaller amplitude the trigger tended to arrive later so that the waveform moved left in the trace window.  This dependence was measured by tracking the start of the pulse in the trace window versus amplitude and fitting it to an empirical polynomial (see Figure~\ref{fig:TriggerPositionDependence}).  This information was folded back into the simulation to control the starting point of the pulse given its amplitude.
	
					\begin{figure}
						\centering
						\includegraphics[width=0.9\textwidth]{chan4amplitude_vs_chan1}
						\caption[Comparison between the amplitudes in the unshaped and shaped BeGe channels]
						{Comparison between the amplitudes in the unshaped channel 4 and shaped channel 1.  
						The line is a linear fit to the data.}
						\label{fig:Risetimechan2vschan4}.
					\end{figure}
					
					\begin{figure}
						\centering
						\includegraphics[width=0.9\textwidth]{start_pulse_vs_energy}
						\caption[Comparison between the start of the preamp pulse rise-time and the amplitude of the shaped channel for the BeGe]
						{Comparison between the start of the preamp pulse rise-time and the amplitude of the shaped
						 channel.  The line is a fourth-order polynomial fit which is used to parameterize this relationship.}
						\label{fig:TriggerPositionDependence}.
					\end{figure}					
	
	Once the simulated pulses were generated, they were run through the same analysis chain as the waveforms from the detector, in particular through the rise-time calculation algorithms described earlier in this section.  The amplitude of the pulses was sampled according to the triggering efficiency measured in Section~\ref{sec:BeGeTrigEff}.  Two simulations were run, one for each the high- and low-gain set of channels, for $\sim$5M events.  These results were then used to calculate contours of particular acceptances, 40, 50, 60, 70, 80, 90, 95, and 99\%.  Results of the simulation for the low-gain channel are shown in Figure~\ref{fig:RisetimeSimulation} along with a calculated 99\% acceptance contour.  The acceptance region defined as all waveforms having a calculated rise-time less than or equal to the calculated contour.  
					
				\begin{figure}
					\centering
					\includegraphics[width=0.9\textwidth]{risetime_cut_chan5_nice}
					\caption[Simulation of calculated rise-time for the low-gain channel of the BeGe]
					{Simulation of calculated rise-time for the low-gain channel of the BeGe.}
					\label{fig:RisetimeSimulation}
				\end{figure}	
	
				\begin{figure}
					\centering
					%\includegraphics[width=0.9\textwidth]{risetime_cut_chan5_nice}
					\caption[Measured rise-time versus energy with a calculated rise-time cut of 99\% acceptance]
					{Measured rise-time versus energy.  The line is a calculated rise-time cut of 99\% acceptance, removing events that fall above the line.}
					\label{fig:RisetimeDataVsCut}
				\end{figure}	
	
		\subsubsection{Risetime systematic tests}
		\label{sec:RisetimeSystematicTests}	
	
	Systematic tests of the cuts were performed to verify the simulation performed adequately and that the cuts behaved as expected.  To do this, cuts of different acceptance were applied to the data set, and an unbinned maximum likelihood fit of the cut data set was performed to measure how the cut affected the different features in the data.  This fit included all x-ray lines (see Table~\ref{tab:XRayLines}) and a background with a flat component and an exponential component.  The fit equation was then:

					\begin{equation}
						b_{1} \exp\left(c_{1} E\right) + b_{2} + \frac{b_{3}}{2}\operatorname{erfc}\left( \frac{E - \mu_{Ge}}{\sqrt{2} \sigma_{Ge}}\right) + 
							\sum^{xrays}_{i} \frac{a_{i}}{\sigma_{i}\sqrt{2 \pi}} 
							\exp\left(-\frac{(E - \mu_{i})^{2}}{2 \sigma_{i}^{2}}\right)
						\label{eqn:InitialFitEqn}
					\end{equation}

where $b_{1}$, $b_{2}$ and $b_{3}$are the exponential, flat, and low-energy (erf) flat background amplitudes respectively, $c_{1}$ is the exponential constant and the sum is over the xray lines present in the fit.  The amplitudes of all components ($a_{i}$, $b_{1}$, $b_{2}$, $b_{3}$) and the exponential constant were allowed to float independently.  This equation does not explicitly include normalization terms, but each component was normalized so that the amplitudes corresponded to counts in for each component.  The parameters ($\mu_{i}$ and $\sigma_{i}$) of the x-ray lines were allowed to float in a small range around their expected values, but the parameters for the error function, $\mu_{Ge}$ and $\sigma_{Ge}$, were defined as 10.367~keV and 0.1~keV, respectively.  The error function parameterizes the `plateau' below the Ge K-capture peak due to partial charge collection.  In general, sets of these error functions should be included for each prominent x-ray line, but since the Ge K-capture line dominates, only one error function centered on the Ge K-capture line was included.

These fits were performed for data sets with the entire set of cuts, including:

					\begin{enumerate}
						\item only LN fills
						\item LN fills and microphonics cuts
						\item LN fills, microphonics cuts, and risetime cuts varying between 40\% and 99\%.
					\end{enumerate}

Fits were performed for both high-energy and low-energy channels, an example of a fit for each channel is given in Figure~\ref{fig:BeGeFitExample}.  A calculation of the relative percentage remaining for each component was made using the cut LN-fills-plus-microphonics as a reference.  This calculation gives a metric for determining the validity of each cut; for example, a rise-time cut with a calculated 70\% acceptance efficiency should retain 70\% of the counts in the x-ray lines and remove an unknown but larger percentage of the counts in the background components (flat plus exponential).  Additionally, the extracted fit value for the exponential constant, $c_{1}$, of the background provides a test of the cut model and can act as a probe of the shape of the background distribution.  
	
					\begin{figure}
						\centering
						\subfigure[Low-gain channel] {
							\includegraphics[width=0.9\textwidth]{HighEnergyFitRun_all_99}					
						}
						\subfigure[High-gain channel] {
							\includegraphics[width=0.9\textwidth]{LowEnergyFitRun_all_99}
						}
						
						\caption[Example of fits performed to estimate amount cut by rise-time cuts]
						{Example of fits performed to estimate amount cut by rise-time cuts.  These data had the 99\% rise-time cut applied.}
						\label{fig:BeGeFitExample}
					\end{figure}
	
					\begin{table}
					\centering
						\begin{tabular}{l|r}
							Isotope & Energy \\
							\hline
							\hline
							$^{65}$Zn L-capture & 1.1~keV \\
							\hline
							$^{68,71}$Ge L-capture & 1.299~keV \\
							\hline
							$^{65}$Zn K-capture & 8.979~keV \\
							\hline
							$^{68,71}$Ga K-capture & 9.659~keV \\
							\hline
							$^{68,71}$Ge K-capture & 10.367~keV \\
							\hline
							$^{73,74}$As K-capture & 11.103~keV \\
							\hline
							\hline
						\end{tabular}	
						\caption[Summary of prominent x-ray lines in the BeGe data set]
						{Summary of prominent x-ray lines in the data set.}
						\label{tab:XRayLines}
					\end{table}


					\paragraph{High-energy results}

The results for the low-gain channel are given in Figure~\ref{fig:RTSimLowGainResults}.  The higher-energy x-rays ($>8$~keV) behave well at the higher acceptance percentages ($\ge$90\%), though the agreement becomes worse at lower acceptances as the lines tend to retain a larger percentage of counts than expected.  The lower-energy L-capture lines of \gersixeight~and \znsixfive~follow the general trend, but the wide error bars due to the large covariance of the parameters of these lines makes it difficult to conclude anything more specific.  This large covariance is due to the lack of resolution of the low-gain channel at these low energies, so the high-gain channel is required to test this cut.  The amplitudes of the background components also behave as expected, showing a reduction significantly greater than 99\% for a 99\%-acceptance risetime cut.  The fit exponential constant, $c_{1}$, demonstrates a marked shift when the rise-time cut is initially implemented, but is consistent between the different rise-time cuts (see Figure~\ref{fig:RTLowGainExpConstant}).  This suggests that the calculated acceptance contours are consistent near threshold.  


						\begin{sidewaysfigure}
							\centering
							\subfigure[Exponential Constant, $c_{1}$l]{
								\includegraphics[width=0.46\textheight]{ExpConstant3}
								\label{fig:RTLowGainExpConstant}
							}
							\subfigure[X-rays]{
								\includegraphics[width=0.46\textheight]{GammaLines6}
								\label{fig:RTLowGainXrays}						
							}
							\subfigure[Low x-rays]{
								\includegraphics[width=0.46\textheight]{LowHighGammaLines0}
								\label{fig:RTLowGainLowXrays}						
							}												
							\subfigure[Background components]{
								\includegraphics[width=0.46\textheight]{BackgroundComponents6}
								\label{fig:RTLowGainBkgd}						
							}														
							\caption[Behavior of fit components after cuts for low-gain channel]
							{Behavior of fit components after cuts for low-gain channel.}
							\label{fig:RTSimLowGainResults}
						\end{sidewaysfigure}
						
						\begin{sidewaystable}
							\centering
							\begin{tabular}{l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  }
								\toprule 
								& \multicolumn{14}{c}{Components (counts)} \\
								\cmidrule[1pt]{2-14} 
								Cut & \multicolumn{2}{c}{$^{65}$Zn (8.98 keV)} & \multicolumn{2}{c}{$^{68}$Ga (9.66 keV)} & \multicolumn{2}{c}{$^{68}$Ge (10.37 keV)} & \multicolumn{2}{c}{$^{73}$As (11.1 keV)} & \multicolumn{2}{c}{Erf bkgd} & \multicolumn{2}{c}{Exp bkgd} & \multicolumn{2}{c}{Flat bkgd}  \\
								\midrule
							        	 Risetime 20\% & 517.2 & 45.4 & 517.2 & 45.4 & 517.2 & 45.4 & 517.2 & 45.4 & 517.2 & 45.4 & 517.2 & 45.4 & 517.2 & 45.4  \\
								 Risetime 30\% & 559.9 & 47.8 & 559.9 & 47.8 & 559.9 & 47.8 & 559.9 & 47.8 & 559.9 & 47.8 & 559.9 & 47.8 & 559.9 & 47.8  \\
								 Risetime 40\% & 609.1 & 50.1 & 609.1 & 50.1 & 609.1 & 50.1 & 609.1 & 50.1 & 609.1 & 50.1 & 609.1 & 50.1 & 609.1 & 50.1  \\
								 Risetime 50\% & 634.5 & 50.9 & 634.5 & 50.9 & 634.5 & 50.9 & 634.5 & 50.9 & 634.5 & 50.9 & 634.5 & 50.9 & 634.5 & 50.9  \\
								 Risetime 60\% & 667.3 & 51.9 & 667.3 & 51.9 & 667.3 & 51.9 & 667.3 & 51.9 & 667.3 & 51.9 & 667.3 & 51.9 & 667.3 & 51.9  \\
								 Risetime 70\% & 670.8 & 52.7 & 670.8 & 52.7 & 670.8 & 52.7 & 670.8 & 52.7 & 670.8 & 52.7 & 670.8 & 52.7 & 670.8 & 52.7  \\
								 Risetime 80\% & 701.7 & 54.0 & 701.7 & 54.0 & 701.7 & 54.0 & 701.7 & 54.0 & 701.7 & 54.0 & 701.7 & 54.0 & 701.7 & 54.0  \\
								 Risetime 90\% & 715.2 & 56.1 & 715.2 & 56.1 & 715.2 & 56.1 & 715.2 & 56.1 & 715.2 & 56.1 & 715.2 & 56.1 & 715.2 & 56.1  \\
								 Risetime 95\% & 726.4 & 55.9 & 726.4 & 55.9 & 726.4 & 55.9 & 726.4 & 55.9 & 726.4 & 55.9 & 726.4 & 55.9 & 726.4 & 55.9  \\
								 Risetime 99\% & 737.0 & 57.7 & 737.0 & 57.7 & 737.0 & 57.7 & 737.0 & 57.7 & 737.0 & 57.7 & 737.0 & 57.7 & 737.0 & 57.7  \\
								 LN + micro & 1139.9 & 71.8 & 1139.9 & 71.8 & 1139.9 & 71.8 & 1139.9 & 71.8 & 1139.9 & 71.8 & 1139.9 & 71.8 & 1139.9 & 71.8  \\
								 
								\bottomrule
							\end{tabular}
							\caption[Behavior of fit components after cuts for low-gain BeGe channel]
							{Behavior of fit components after cuts for low-gain channel.  Components are given in total counts.}
							\label{tab:RTLowGainResults}
						\end{sidewaystable}						

					\paragraph{Low-energy results}

Results for the low-energy channel are shown in Figure~\ref{fig:RTSimHighGainResults}. The conclusions for this channel set are similar to the high-energy channel set.  The enhanced resolution of the high-gain channel has reduced the size of the error bars for the L-capture lines and the reduction of the amplitudes of the lines is consistent with the expected acceptances.  The amplitudes of the different background components agree with the amplitudes calculated for the high-energy channel.  The larger error bars on both the background components are because the flat component has fewer constraints: the size of the flat region is significantly smaller in the high-gain channel than in the low-gain channel.  


						\begin{sidewaysfigure}
							\centering
							\subfigure[Exponential Constant, $c_{1}$l]{
								\includegraphics[width=0.46\textheight]{LowExpConstant1}
								\label{fig:RTHighGainExpConstant}
							}
							\subfigure[X-rays]{
								\includegraphics[width=0.46\textheight]{LowGammaLines3}
								\label{fig:RTHighGainXrays}						
							}		
							\subfigure[Background components]{
								\includegraphics[width=0.46\textheight]{LowBackgroundComponents3}
								\label{fig:RTHighGainBkgd}						
							}														
							\caption[Behavior of fit components after cuts for high-gain BeGe channel]
							{Behavior of fit components after cuts for high-gain channel.}
							\label{fig:RTSimHighGainResults}
						\end{sidewaysfigure}
						
						\begin{sidewaystable}
							\centering
							\begin{tabular}{l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  r@{$~\pm~$}l  }
								\toprule 
								& \multicolumn{8}{c}{Components (counts)} \\
								\cmidrule[1pt]{2-8} 
								Cut & \multicolumn{2}{c}{$^{65}$Zn (1.1 keV)} & \multicolumn{2}{c}{$^{68}$Ge (1.3 keV)} & \multicolumn{2}{c}{Exp bkgd} &
																 \multicolumn{2}{c}{Flat bkgd}  \\
										\midrule
						        	 	Risetime 20\% & 166.7 & 17.6 & 166.7 & 17.6 & 166.7 & 17.6 & 166.7 & 17.6  \\
								Risetime 30\% & 203.8 & 19.1 & 203.8 & 19.1 & 203.8 & 19.1 & 203.8 & 19.1  \\
								 Risetime 40\% & 234.6 & 20.5 & 234.6 & 20.5 & 234.6 & 20.5 & 234.6 & 20.5  \\
								 Risetime 50\% & 264.0 & 21.9 & 264.0 & 21.9 & 264.0 & 21.9 & 264.0 & 21.9  \\
								 Risetime 60\% & 283.8 & 22.7 & 283.8 & 22.7 & 283.8 & 22.7 & 283.8 & 22.7  \\
								 Risetime 70\% & 308.7 & 23.8 & 308.7 & 23.8 & 308.7 & 23.8 & 308.7 & 23.8  \\
								 Risetime 80\% & 337.5 & 24.5 & 337.5 & 24.5 & 337.5 & 24.5 & 337.5 & 24.5  \\
								 Risetime 90\% & 367.7 & 26.1 & 367.7 & 26.1 & 367.7 & 26.1 & 367.7 & 26.1  \\
								 Risetime 95\% & 379.0 & 26.7 & 379.0 & 26.7 & 379.0 & 26.7 & 379.0 & 26.7  \\
								 Risetime 99\% & 419.4 & 29.6 & 419.4 & 29.6 & 419.4 & 29.6 & 419.4 & 29.6  \\
								 LN + micro & 777.4 & 63.8 & 777.4 & 63.8 & 777.4 & 63.8 & 777.4 & 63.8  \\
								 
								\bottomrule
							\end{tabular}
							\caption[Behavior of fit components after cuts for high-gain BeGe channel]
							{Behavior of fit components after cuts for high-gain channel.  Components are given in total counts.}
							\label{tab:RTHighGainResults}
						\end{sidewaystable}

		\subsubsection{Conclusions}
		\label{sec:RTBeGeConclusions}
		
FixME - Add discussion on Rise-time cuts regarding its interpretation as a fiducial volume cut and the estimate of the mass difference that went along with this.

Evidence in past results from~\cite{Strauss196780,Sakai:1971ff} and in recent results presented in~\cite{Aalseth:2010aa} suggest that slow pulses can arise due to poor charge collection from weak electric fields in the dead layer of the n$^{+}$ Li contact.  These results also suggest that energy information of the initiating event is lost so that these slow pulses can comprise a background at low energy, though one of not-precisely-determined distribution.	 These conclusions were determined by scanning a source of low-energy gammas (\amtwofourone, 59.5~keV gamma) around the outside n$^{+}$ and determining the distribution of rise-times for recorded events.  The low energy of the americium source ensured that a majority of the gammas deposited energy in or near the outer contact.  The resulting rise-time measurements demonstrated a substantial distribution of slow rise-time pulses, supporting the conclusion that at least a component of these slow-rise-time events come from energy deposition at or near the outer contact.  

Another measurement performed using a separate, similar PPC detector~\cite{BarbThesis} measured the rate versus time at low energies in several energy regions, watching the decay of \gersevenone~(11.4~day half-life).  This measurement focused on three main regions: the Ge K-capture line (10.367~keV), the Ge L-capture line (1.3~keV), and the flat region in between ($2\to6$~keV).  The results found that the decay of counts in the flat region matched the decays of counts in the L- and K-capture regions, suggesting partial energy deposition from the \gersevenone~decay below 10.367~keV.  It is likely that this result is related to the slow-rise-time pulses, in particular that decays of \gersevenone~near the outer contact resulted in both slow pulses and partial charge collection, but no preamp traces were taken during this measurement.  To firmly establish the equivalency of these results, it is essential to measure the rise-time of the preamplifier traces, remove slow-pulses, and produce the resulting counts vs.~time for the region between the L- and K-capture lines.  If partial energy deposition is arising in the same process producing the slow-pulses, the removal of slow-pulse events should result in the flattening of the counts vs.~time curve for the energy region between the capture lines.  Unfortunately, for this analysis preamplifier traces were only recorded following the decay of the majority of \gersevenone~and the slow decay of \gersixeight~(271~days) yields insufficient statistics over a short period of time to measure this well.  The relative content of \gersevenone~in the detector can be increased through activation by either bringing the detector to the surface or introducing an neutron source, but such a test cannot be performed until the conclusion of the counting runs.  

Whereas it is likely that a component of these slow-pulses is arising from charge deposition near the crystal outer contact, it is important to determine other possible sources of these events.  For example, surface channel effects, i.e.~distortion of crystal fields through charge collection on the passivated surface, might produce similar results.  One could use the extreme sensitivity of the surface channel to temperature detailed in~\cite{Hull1995488} to determine if the population of slow-rise-time events changes with temperature.  Additionally, characterization measurements using a low-energy gamma source must be performed for several other PPC detectors to determine how sensitive the distribution of slow-rise-time events is to manufacturing and geometry differences.  

Whatever the origin of these slow-pulse events, it is clear that they can produce a background to low-energy signals.  If the majority of these slow pulses arise from energy deposition near the outer surface, a cut on rise-time would effectively generate a cut on the fiducial volume and mass of the detector.  Understanding the active fiducial volume of the detector is critical for experiments sensitive to mass exposure times, such as those searching for $\nonubb$ or dark matter.  Even though $\qval$ is around 2~MeV, the decay is internal to the detector and energy is deposited by two electrons summing to $\qval$.  The range of these electrons is on the order of 1~mm, so it is expected that this could still have an effect on $\nonubb$ results.  

For this analysis and for the purpose of generating exclusion plots, we assume that rise-time events 

	\section{Time correlations and systematics}
	\label{sec:BeGeTimeCorrelations}

	Searching for time dependence of parameters and any possible time correlation of events provides a powerful systematic test of the data.  Small deviations in detector health (e.g.~noise, baseline shifting, etc.) can have a more marked effect on events near threshold.  For example, a change in noise or a shift in baseline could affect the triggering efficiency, modifying the collection probability for events near threshold.  Additionally, searching for time correlations provides a metric for ensuring that data collection is behaving appropriately and as expected.  In addition to tracking detector health parameters, it is important to analyze event rates and times between events to look for any deviation from Poisson statistics.  Such a deviation could arise from environmental changes or events.  For example, nearby work in the experimental hall generating additional vibration could increase microphonics events, or an introduction of a hot source in the shared lab space could increase count rates.  Finally, it is possible to perform tests in combined energy-time space looking to see if classes of events (e.g.~events in a certain energy range) have correlations \emph{outside} their class region.  An obvious example of this is a decay chain, in which an initial energy deposition from a parent is correlated in time with another event at different energies from the decay of the daughter.  Whereas these types of correlations between decay chains should show up in the data, the discovery of other correlations of unknown origin could point to a potential source of event contamination.  
	
		\subsection{Detector health versus time}
		\label{sec:BeGeParsVsTime}

	In contrast to the previous data run with PPC2, fewer parameters were tracked during the lifetime of the experiment (see Section~\ref{sec:PPC2DetParsVsTime}).  In particular, due to limited channels in the DAQ electronics the trigger efficiency was not monitored with automated tests nor was the rate of the reset electronics recorded.  In general, both these parameters are important to catalogue since they could affect the signal region near threshold.  However, due to the strong correlation of the reset rate on  the measured baseline in unshaped pulses seen in the results of Section~\ref{sec:PPC2DetParsVsTime},  this suggested that the baseline could be used as a proxy to ensure that the reset rate did not change significantly.  Similarly, it was possible to monitor the noise by using the measured RMS of the baseline to determine whether it changed significantly over time.  
	
	The baseline of two channels was tracked, channel 1 (shaped) and channel 5 (unshaped preamp trace).  For every day of run-time, the calculated baselines were binned and fit to a gaussian.  As discussed before in Section~\ref{sec:BeGeBaseline}, the binned baseline data included outliers in one direction which were attributed to having come from the return of the baseline to its nominal value following a reset event in the preamplifier.  Since these outliers were a low percentage of the total events, only one gaussian was fit to the binned baseline data.  From these fits, the mean and $\sigma$ values were extracted and the relationship of these parameters versus time are plotted in Figure~\ref{fig:BeGeBaselineData}.  An inspection of the results reveals that the $\sigma$ values from both unshaped and shaped channels were insensitive to variation.  In contrast, the mean value of the baseline in the unshaped channel demonstrates variation, but this is not mirrored exactly in the mean of the baseline calculated from the shaped channel.  This is likely due to baseline restoration electronics in the spectroscopy amplifier which render the baseline of the output less sensitive to baseline modulation of the input.  Regardless, it is evident that the noise of the baseline does not change significantly in either the shaped or unshaped channels over the time range of the experiment.  This suggests that the trigger efficiency and electronic noise remained similarly unchanged throughout the time of the experiment.  

			\begin{sidewaysfigure}
				\centering
				\def\figwidth{0.45\textheight}
				\newcommand{\plotname}[2]{Baseline#1haped#2}			
				\subfigure[Unshaped channel, mean]{
					\includegraphics[width=\figwidth]{\plotname{Un}{Mean}}
					\label{fig:\plotname{Un}{Mean}}
				}				
				\subfigure[Unshaped channel, $\sigma$]{
					\includegraphics[width=\figwidth]{\plotname{Un}{Sigma}}
					\label{fig:\plotname{Un}{Sigma}}
				}
				\subfigure[Shaped channel, mean]{
					\includegraphics[width=\figwidth]{\plotname{S}{Mean0}}
					\label{fig:\plotname{S}{Mean}}
				}				
				\subfigure[Shaped channel, $\sigma$]{
					\includegraphics[width=\figwidth]{\plotname{S}{Sigma0}}
					\label{fig:\plotname{S}{Sigma}}
				}		
				\caption[Baseline mean and $\sigma$ versus time]
				{Baseline mean and $\sigma$ versus run time for both shaped and unshaped preamp traces.}
				\label{fig:BeGeBaselineData}
			\end{sidewaysfigure}
	
		\subsection{Rates in energy regions}
		\label{sec:BeGeRate}	
		
		Several tests can be performed to analyze the Poisson nature of the data: (1) event rate in an energy region; (2) time difference between events in an energy region; and (3) forward and backward time difference between events occurring in an energy region and \emph{any} other events.  All of these tests should be made with different cuts applied (see Section~\ref{sec:BeGeCuts} for a description of the cuts) since an analysis cut may preferentially remove a portion or an entire population of correlated events.  Therefore, for these tests several cuts were used including the minimal cut (LN fill cut), Microphonics cuts and Rise-time cuts of varying acceptances.  To perform all these tests, events were selected in particular energy regions: $0.45\to0.55$ keV, $0.5\to1$ keV, $1\to2$ keV, $3\to8$ keV, and $10\to10.76$ keV.  The first two regions were to test around and just above threshold, the next two to provide test regions not significantly populated with $\epsilon$ decays, and the final as a test region symmetric around the Ge K-capture x-ray (10.367~keV).
		
	During LN fills, all events are vetoed and this could, in principal, generate a significant distortion to any calculated poisson rates.  However, since the amount of time vetoed during an LN fill is small compared to the measurement time this has little effect on the results.  This can be seen by considering that the probability that one or more events will happen in a certain time period, $\tau$, is $1 - \exp(-\lambda_{\tau})$, where $\lambda_{\tau}$ is the average count rate over that time period.  If we consider that the time vetoed during an LN fill is $\sim15$~minutes, and these fills happen once every two days, the expected number of times one or more events occurs during the roughly 160 day run period is $\sim80 (1 - \exp(-\lambda_{\tau}) )$.  Additionally, the percentage of events affected (events affected/events expected) is given by $\frac{1}{192 \lambda}\left(1 - \exp(-\lambda)\right)$.  The total rate for any of this regions is less than 1 count per 15 minutes, meaning that the percentage of events affected is less than 0.5\%.  Since all other cuts (microphonics and rise-time cuts) should have no time dependence, it is expected that rates calculated from data with these cuts applied should be slower, but still Poisson in nature.  
	
	The first test involves analyzing the event rates in a given energy window.  The data was windowed in 8-hour bins and the number of counts in the different energy regions was calculated for each of these 8 hours.  Any 8-hour time periods with less than 100\% live-time (i.e.~8 hour runs during which an LN fill occurred) were removed from the data set.  The number-of-counts-seen was histogrammed and fit with a Poisson distribution to extract the rate.  Examples of these fits are shown in	Figure~\ref{fig:BeGeRateFits} and a summary of the fits, including extracted rate and goodness-of-fit, is given in Table~\ref{tab:BeGeFitRates}.  These results indicate that the regions selected behave as expected, demonstrating count rates that follow Poisson distributions.  Additionally, as expected the count rates reduced with the introduction of cuts while retaining a Poisson shape.  
	
			\begin{table}
				\centering
				\begin{tabular}{r@{$~\to~$}l| r@{$~\pm~$}l | c | c | c} 
					\multicolumn{2}{c|}{Range (keV)} & \multicolumn{2}{c|}{$\lambda$ (cts / hr)} & $\chi^2$ & NDF & P-Value \\
					\hline
					\multicolumn{7}{l}{LN} \\
					\hline					
					0.45 & 0.55 & 0.046 & 0.004 & 6.08296 & 2 & 0.0477643 \\
					0.5 & 1 & 0.158 & 0.006 & 2.73425 & 5 & 0.740876 \\
					1 & 2 & 0.262 & 0.010 & 4.34057 & 7 & 0.73982 \\
					3 & 8 & 0.373 & 0.010 & 4.94593 & 9 & 0.838996 \\
					10 & 10.76 & 0.609 & 0.016 & 15.5406 & 12 & 0.213196 \\
					\hline
					\multicolumn{7}{l}{LN + Microphonics} \\
					\hline
					0.45 & 0.55 & 0.041 & 0.004 & 1.78836 & 2 & 0.408943 \\
					0.5 & 1 & 0.143 & 0.006 & 3.82657 & 5 & 0.574646 \\
					1 & 2 & 0.204 & 0.008 & 1.40094 & 6 & 0.965801 \\
					3 & 8 & 0.341 & 0.010 & 7.37705 & 8 & 0.496551 \\
					10 & 10.76 & 0.563 & 0.016 & 14.9226 & 11 & 0.186068 \\
					\hline					
					\multicolumn{7}{l}{LN + Microphonics + 99\% Rise-time cuts} \\
					\hline
					0.45 & 0.55 & 0.038 & 0.004 & 2.57558 & 2 & 0.27588 \\
					0.5 & 1 & 0.124 & 0.006 & 6.25875 & 5 & 0.281849 \\
					1 & 2 & 0.139 & 0.006 & 3.6481 & 5 & 0.601106 \\
					3 & 8 & 0.221 & 0.008 & 2.73918 & 6 & 0.840798 \\
					10 & 10.76 & 0.544 & 0.015 & 15.0561 & 11 & 0.179943 \\
					\hline
				\end{tabular}
				\caption[Count rates in 8-hour time periods for selected energy ranges]
				{Count rates in 8-hour time periods for selected energy ranges.  Results from data with LN, LN+microphonics, and LN+microphonics+99\% 
				rise-time cut are included.  $\lambda$ is normalized to counts per hour.  The P-Value is the probability of observing this $\chi^{2}$ value or greater
				given the numbers-of-degrees-of-freedom (NDF).}
				\label{tab:BeGeFitRates}
			\end{table}

			\begin{sidewaysfigure}
				\centering
				\def\figwidth{0.3\textheight}
				\def\plotname{LN_Poisson_Region}
				\subfigure[$0.45\to0.55$~keV]{
					\includegraphics[width=\figwidth]{\plotname_0.45_0.55}
					\label{fig:\plotname45to55}
				}			
				\subfigure[$0.5\to1$~keV]{
					\includegraphics[width=\figwidth]{\plotname_0.5_1}
					\label{fig:\plotname5to1}
				}
				\subfigure[$1\to2$~keV]{
					\includegraphics[width=\figwidth]{\plotname_1_2}
					\label{fig:\plotname1to2}
				}								
				\subfigure[$3\to8$~keV]{
					\includegraphics[width=\figwidth]{\plotname_3_8}
					\label{fig:\plotname3to8}
				}
				\subfigure[$10\to20.76$~keV]{
					\includegraphics[width=\figwidth]{\plotname_10_10.76}
					\label{fig:\plotname10to1076}
				}								
				\caption[Fits to count rates in 8-hour time periods for selected energy ranges]
				{An example of fits to count rates in 8-hour time periods for selected energy ranges.  LN cuts were applied to the data.}
				\label{fig:BeGeRateFits}
			\end{sidewaysfigure}

	The time between adjacent events was calculated for events in selected energy regions.  The differences were then binned and the result fit to an exponential to extract the decay parameter, $\lambda$, which is equivalent to the parameter extracted in the previous test.  Because of this, analyzing the time-difference is essentially equivalent to the analyzing the event rate, but it can have different sensitivities since it is not confined to a particular time range.  In particular, the previous test is not sensitive to correlations beyond 8 hours, whereas a time-difference test is not limited to a specific time range.  Results of this test are presented as before, with examples of fits to LN-cut data given in Figure~\ref{fig:BeGeExpFits} and extracted parameters given in Table~\ref{tab:BeGeFitExp}.  It is clear that the P-value of the fit for $10 \to 10.76$~keV region is low, or rather close to one of the bounds (0 or 1) and, in contrast to the other regions, does not significantly change as one would expect with the application of different cuts.  No corrections for LN fills have been introduced for these tests because doing so would require assuming an underlying Poisson distribution.  However, since the count rate in the \gersixeight ~region is higher than in the other regions, it is possible that this has distorted the distribution enough to increase the $\chi^{2}$.  Despite this, all the rate values calculated using this method are consistent with the rates calculated using the previous method, lending confidence to the conclusion that the count rates are behaving as Poisson variables.

			\begin{table}
				\centering
				\begin{tabular}{r@{$~\to~$}l  r@{$~\pm~$}l   c  c  c} 
					\multicolumn{2}{c}{Range (keV)} & \multicolumn{2}{c}{$\lambda$ (cts)} & $\chi^2$ & NDF & P-Value \\
					\hline
					\multicolumn{7}{l}{LN} \\
					\hline
					0.45 & 0.55 & 0.061 & 0.008 & 23.9883 & 27 & 0.630963 \\
					0.5 & 1 & 0.172 & 0.007 & 22.3103 & 16 & 0.133452 \\
					1 & 2 & 0.266 & 0.009 & 5.42462 & 13 & 0.964619 \\
					3 & 8 & 0.380 & 0.010 & 13.4823 & 15 & 0.5651 \\
					10 & 10.76 & 0.638 & 0.016 & 24.603 & 12 & 0.0168202 \\
					\hline
					\multicolumn{7}{l}{LN + Microphonics} \\
					\hline
					0.45 & 0.55 & 0.049 & 0.008 & 20.021 & 27 & 0.829872 \\
					0.5 & 1 & 0.152 & 0.007 & 15.6124 & 18 & 0.61958 \\
					1 & 2 & 0.207 & 0.008 & 7.90211 & 13 & 0.849919 \\
					3 & 8 & 0.353 & 0.010 & 12.2548 & 19 & 0.874449 \\
					10 & 10.76 & 0.593 & 0.015 & 24.9871 & 13 & 0.0231737 \\
					\hline
					\multicolumn{7}{l}{LN + Microphonics + 99\% Rise-time cuts} \\
					\hline
					0.45 & 0.55 & 0.045 & 0.009 & 23.0667 & 26 & 0.62917 \\
					0.5 & 1 & 0.133 & 0.007 & 14.4484 & 19 & 0.756952 \\
					1 & 2 & 0.139 & 0.008 & 16.5919 & 16 & 0.412466 \\
					3 & 8 & 0.231 & 0.010 & 12.3232 & 20 & 0.904504 \\
					10 & 10.76 & 0.577 & 0.015 & 27.4457 & 14 & 0.0168382 \\
					\hline
				\end{tabular}
				\caption[Results of fits to time differences between events in selected energy ranges]
				{Results of fits to time differences between events in selected energy ranges.  Results from data with LN, LN+microphonics, and LN+microphonics+99\% 
				rise-time cut are included.  $\lambda$ is measured in counts per \emph{hour}, in contrast to results in Table~\ref{tab:BeGeFitRates}. 
				The P-Value is the probability of observing this $\chi^{2}$ value or greater given the NDF.}
				\label{tab:BeGeFitExp}
			\end{table}

			\begin{sidewaysfigure}
				\centering
				\def\figwidth{0.3\textheight}
				\def\plotname{LNExponential}
				\subfigure[$0.45\to0.55$~keV]{
					\includegraphics[width=\figwidth]{\plotname_0.45_0.55}
					\label{fig:\plotname45to55}
				}			
				\subfigure[$0.5\to1$~keV]{
					\includegraphics[width=\figwidth]{\plotname_0.5_1}
					\label{fig:\plotname5to1}
				}
				\subfigure[$1\to2$~keV]{
					\includegraphics[width=\figwidth]{\plotname_1_2}
					\label{fig:\plotname1to2}
				}								
				\subfigure[$3\to8$~keV]{
					\includegraphics[width=\figwidth]{\plotname_3_8}
					\label{fig:\plotname3to8}
				}
				\subfigure[$10\to20.76$~keV]{
					\includegraphics[width=\figwidth]{\plotname_10_10.76}
					\label{fig:\plotname10to1076}
				}									
				
				\caption[Fits to time differences between events in selected energy ranges]
				{An example of fits to time differences between events in selected energy ranges.  LN cuts were applied to the data.}
				\label{fig:BeGeExpFits}
			\end{sidewaysfigure}
			
	The final test described in this section involves selecting events in a given energy region and calculating the time differences to the events at \emph{any} energy (below 14~keV) both immediately previous and immediately following.  These time differences are then binned and fit to an exponential to extract the rate.  The purpose of this test is to search for correlations between events occurring in the selected region and any other events, looking for any possible `memory' either forwards or backwards in time.  In other words, the rates calculated for both forward and backward events and for different energy regions should all be consistent and the calculated rate should be the average data rate given a set of cuts.  
	
	Also, by applying it to data with different cuts, it is possible to test the gross efficiency of these cuts since it is essentially a measurement of the count rate.  For example, the rate calculated for data with 70\% rise-time cut applied should be $0.7/0.99\times$ the rate calculated for a 99\% rise-time cut, assuming that the majority of background from slow pulses has been removed.  Results from this test are show in Figure~\ref{fig:BeGeForBack}.  The line in the figure is an expectation of the calculated rate value, determined by taking the average, $\rho$, of the forward and backward rates for the $2\to8$~keV region for the 99\% rise-time-cut data and extrapolating to the lower efficiencies.  That is, the values at other rise-time efficiencies,  $\epsilon$, are calculated by $\frac{\epsilon}{0.99}\rho$.  The results indicate that the forward and backward rates are consistent amongst the three regions tested and that the reduction in rate follows the correct trend.  The scatter of the rate measured for the $0.5\to1$~keV is higher due to the fewer triggers seen in this region.
	
			\begin{figure}
				\centering
				\includegraphics[width=0.7\textwidth]{BeforeAndAfterEvents}
				\caption[Rates calculated from the time difference between events in selected regions]
				{Rates calculated from the time difference between events in selected regions and events outside that region.  The rates were calculated both forward
				in time (squares) and backwards in time (triangles) for different cut types.  The line drawn is an estimate of the expected efficiency of the rise-time cuts, 
				see text for details.}
				\label{fig:BeGeForBack}
			\end{figure}	
			
			\begin{table} \scriptsize
				\centering
				\renewcommand{\arraystretch}{0.75}
				\begin{tabular}{l  c  c  c  r@{$~\pm~$}l  c  c  c } 
					Cut & Range (keV) & Triggers & Direction & \multicolumn{2}{c}{Rate (cts/hr)} & $\chi^2$ & NDF & P-value  \\
					\hline
				\multirow{6}{*}{Risetime 50\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{166}	& B & -1.10 & 0.10 & 4.77 & 7 & 0.689\\
					& & & F & -0.98 & 0.09 & 2.81 & 7 & 0.902\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{651}	& B & -1.03 & 0.05 & 19.10 & 18 & 0.386\\
					& & & F & -0.98 & 0.04 & 13.48 & 20 & 0.856\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1158}	& B & -1.07 & 0.03 & 39.84 & 45 & 0.690\\
					& & & F & -1.07 & 0.03 & 26.00 & 39 & 0.945\\
				\hline
				
				\multirow{6}{*}{Risetime 60\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{205}	& B & -1.20 & 0.10 & 7.52 & 6 & 0.275\\
					& & & F & -1.06 & 0.09 & 2.80 & 7 & 0.902\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{711}	& B & -1.20 & 0.05 & 23.24 & 18 & 0.182\\
					& & & F & -1.15 & 0.05 & 30.70 & 19 & 0.044\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1292}	& B & -1.13 & 0.03 & 28.50 & 39 & 0.892\\
					& & & F & -1.18 & 0.04 & 19.85 & 36 & 0.987\\
				\hline
				
				\multirow{6}{*}{Risetime 70\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{242}	& B & -1.30 & 0.10 & 7.04 & 6 & 0.317\\
					& & & F & -1.17 & 0.09 & 3.23 & 7 & 0.863\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{769}	& B & -1.32 & 0.05 & 14.98 & 17 & 0.597\\
					& & & F & -1.24 & 0.05 & 24.93 & 18 & 0.127\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1482}	& B & -1.28 & 0.03 & 31.32 & 36 & 0.691\\
					& & & F & -1.31 & 0.04 & 28.13 & 36 & 0.823\\
				\hline
				
				\multirow{6}{*}{Risetime 80\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{293}	& B & -1.31 & 0.10 & 3.56 & 5 & 0.615\\
					& & & F & -1.29 & 0.09 & 4.38 & 8 & 0.821\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{824}	& B & -1.43 & 0.05 & 11.13 & 15 & 0.744\\
					& & & F & -1.38 & 0.06 & 22.71 & 16 & 0.122\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1628}	& B & -1.42 & 0.04 & 37.31 & 34 & 0.319\\
					& & & F & -1.44 & 0.04 & 26.58 & 34 & 0.814\\
				\hline
				
				\multirow{6}{*}{Risetime 90\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{350}	& B & -1.54 & 0.10 & 1.89 & 4 & 0.756\\
					& & & F & -1.46 & 0.09 & 4.15 & 7 & 0.762\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{884}	& B & -1.60 & 0.06 & 9.95 & 15 & 0.823\\
					& & & F & -1.52 & 0.06 & 27.90 & 15 & 0.022\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1779}	& B & -1.54 & 0.04 & 33.90 & 32 & 0.376\\
					& & & F & -1.59 & 0.04 & 24.89 & 30 & 0.730\\
				\hline
				
				\multirow{6}{*}{Risetime 95\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{402}	& B & -1.69 & 0.09 & 2.74 & 4 & 0.602\\
					& & & F & -1.56 & 0.09 & 3.80 & 6 & 0.704\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{914}	& B & -1.70 & 0.06 & 10.25 & 14 & 0.744\\
					& & & F & -1.62 & 0.07 & 30.54 & 15 & 0.010\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1872}	& B & -1.62 & 0.04 & 28.90 & 32 & 0.624\\
					& & & F & -1.69 & 0.04 & 23.45 & 27 & 0.661\\
				\hline
				
				\multirow{6}{*}{Risetime 99\%}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{457}	& B & -1.94 & 0.10 & 5.89 & 4 & 0.207\\
					& & & F & -1.69 & 0.09 & 1.94 & 5 & 0.858\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{976}	& B & -1.84 & 0.06 & 11.25 & 13 & 0.590\\
					& & & F & -1.77 & 0.07 & 24.01 & 14 & 0.046\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{1982}	& B & -1.74 & 0.04 & 24.28 & 28 & 0.666\\
					& & & F & -1.90 & 0.05 & 32.06 & 27 & 0.230\\
				\hline
				
				\multirow{6}{*}{LN + micro}& \multirow{2}{*}{$0.5~\to~1$} & \multirow{2}{*}{526}	& B & -2.37 & 0.11 & 8.35 & 4 & 0.080\\
					& & & F & -2.11 & 0.10 & 1.92 & 4 & 0.751\\
				& \multirow{2}{*}{$2~\to~8$} & \multirow{2}{*}{1564}	& B & -2.10 & 0.06 & 11.87 & 11 & 0.374\\
					& & & F & -2.13 & 0.06 & 23.39 & 10 & 0.009\\
				& \multirow{2}{*}{$10~\to~10.76$} & \multirow{2}{*}{2054}	& B & -2.14 & 0.05 & 26.70 & 24 & 0.319\\
					& & & F & -2.29 & 0.05 & 29.52 & 22 & 0.131\\
				\hline
				
					\hline
				\end{tabular}


				\caption[Results of fits to time differences for events in selected energy ranges with different cuts]
				{Results of fits to time differences between events in selected energy ranges with different cuts.  
				The P-Value is the probability of observing this $\chi^{2}$ value or greater given the NDF.}
				\label{tab:BeGeForBackTable}			
			\end{table}

		\subsection{Time-energy correlations}
		\label{sec:BeGeTimeEnergyCor}

	Two-dimensional energy-time correlations were calculated by analyzing a time window around events occurring in a selected region.  As before, a set of events were selected within an energy range to be used as `trigger' events.  Given the time of a trigger event as a reference point, a two-dimensional energy vs.~time histogram was generated for all events occurring within a time window forwards and backwards around the reference.  The 2-d histograms for each trigger were then added together, producing a final intensity plot.  Any strong correlation between events in energy-time space should show up as having significant intensity in these figures.  The data used for this analysis had only LN fill cuts applied to avoid removing correlating events from the data set.
	
	To give an example of how this calculation works, the time-energy correlation between the \gersixeight~and \galsixeight~decays was analyzed.  The details of this decay chain are discussed in Section~\ref{sec:Ge68ProdPPC}, but a summary of the relevant points is given:  \gersixeight~internal to the crystal decays to \galsixeight, depositing 10.36~keV of energy $\sim90\%$ of the time via x-rays and Auger electrons.  \galsixeight~has a 68~minute half-life and decays via electron-capture $\sim10$\% of the time; 90\% of these decays are K-line captures and deposit 9.7~keV into the crystal.  We can therefore look for a time correlation by choosing the trigger region $10\to10.76$~keV and looking forwards and backward in time around the 9.7~keV decay region of \galsixeight~\cite{Schonfeld1994955}.  Results for 2211~triggers are shown in Figure~\ref{fig:BeGeGeCorrelation} where the intensity of the daughter decay is clear between 0 and 2~hours post trigger.  The rough expected count rate for the Ga K-capture decay is about $0.09\times2211 \sim 200$ which agrees with the results of the plot.  
	

			\begin{figure}
				\centering
				\includegraphics[width=0.7\textwidth]{GeCorrelationHighEnergy4}
				\caption[Time-energy correlation triggering on \gersixeight~decays.]
				{Time-energy correlation triggering on \gersixeight~decays (2211~triggers), z-axis is in counts.  The region intensity centered at 9.7~keV between 0 and 2~hours post trigger arises from the K-capture decay of the 
				\galsixeight~daughter.}
				\label{fig:BeGeGeCorrelation}
			\end{figure}	

	This analysis was applied to other trigger regions, including a region against threshold, $0.5\to1$~keV (, and away from cosmogenic peaks, $2\to6$~keV.  The results are presented in Figure~\ref{fig:BeGeTimeEnergyCor} for an energy window from 0.5 to 8~keV, avoiding higher energies where the count rates from the \znsixfive~and \gersixeight~dominate the intensities.  The only apparent features in these plots are intensities at around 1.3~keV which correspond to random coincidences with the \gersixeight~L-capture decay.  It is possible to integrate these 2-dimensional histograms along the time axis, producing an average energy spectrum around a trigger in the designated region.  By dividing by the number of triggers and the time of integration, one can compare the spectra from different trigger regions to identify any particular discrepancies.  This was done for all three regions presented in this section, integrating over $\pm12$~hours for the lower energy regions and $\pm3$~hours for the \gersixeight~K-capture region.  The results of this integration are shown in Figure~\ref{fig:BeGeIntegratedTimeEnergyCor}.  All of the trigger regions are consistent, except that the $10\to10.76$~keV trigger region demonstrates an enhancement in the \galsixeight~K-capture line.  This is to be expected since the average rate of the Ga decay should be higher centered on a trigger from the parent decay.

			\begin{figure}
				\centering
				\def\figwidth{0.44\textwidth}
				\def\plotname{CorrelationHighEnergy}	
				\subfigure[$0.5\to1$~keV]{
					\includegraphics[width=\figwidth]{NearTrigger\plotname2}
					\label{fig:\plotname5to1}
				}							
				\subfigure[$2\to6$~keV]{
					\includegraphics[width=\figwidth]{Test\plotname1}
					\label{fig:\plotname2to6}
				}			
				
				\caption[Time-energy correlations in selected regions.]
				{Time-energy correlations in selected regions with 575 triggers in the $0.5\to1$~keV region, 1200 in $2\to6$~keV.  The z-axis is in counts}
				\label{fig:BeGeTimeEnergyCor}
			\end{figure}
	
	
			\begin{figure}
				\centering
				\def\figwidth{0.44\textwidth}
				\def\plotname{EnergyProjection0}	
				\subfigure[Low energy]{
					\includegraphics[width=\figwidth]{Low\plotname}
					\label{fig:Low\plotname}
				}							
				\subfigure[High energy]{
					\includegraphics[width=\figwidth]{High\plotname}
					\label{fig:High\plotname}
				}			
				
				\caption[Average energy spectra around triggers in selected energy regions.]
				{Average energy spectra around triggers in selected energy regions.  The $0.5\to1$ and $2\to6$~keV regions were integrated $\pm12$~hours around a trigger, the $10\to10.76$~keV region
				$\pm3$~hours.  As expected, there is a slight enhancement in the \galsixeight~K-capture peak for triggers in the \gersixeight~region.}
				\label{fig:BeGeIntegratedTimeEnergyCor}
			\end{figure}	
			
		\subsection{Conclusions}
		\label{sec:BeGeTimeConclusions}			
		
	\section{Low-energy features}
	\label{sec:BeGeLowEnergyFeatures}
	
	Here, we discuss how the investigation of several parameters in the low-energy region to try and obtain an idea of from where they might arise.

FixME - Flesh out low-energy features, discuss from where these might arise, how one might actually test such hypotheses, and how we got around it for this stage.

	\section{Cuts discussion}
	\label{sec:BeGeCutsDiscussion}

FixME - Add figure showing what is cut out for each cut.

	\section{Conclusions}
     	\label{sec:CutConclusions}		
	

FixME - Discuss the final data sets, the calculation of the total efficiencies, discuss how these may be used (point to the next chapter).  
  
FixME - Discuss possible moves forward in the future, i.e. how to generate a better data set, what sort of data you'd like to take, (measurements), etc.  			
	% At this point, go through and discuss how each of the systematic of each of these cuts must be tested.
	% Discuss how this methodology is later applied to a real dat set.  
	% Discuss other possibilities for reduction in noise, especially with respect to preamp traces.


